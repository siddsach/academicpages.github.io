---
title: "What are we Transferring Anyway?"
excerpt: "Using Attention Weights to Interpret domain choice in Transfer Learning<br/><img src='/images/rnn.png'>"
collection: portfolio
---

#(In Progress) 


[Code](https://github.com/siddsach/Interpreting-Attention)

## Data

### Language Model Datasets
* Wikitext-2
* Gigaword
* Penn Tree Bank

### Text Classification Datasets
* IMDB Sentiment Classification
* MPQA Subjectivity Classification

### Word Vectors
* CharNGram
* Google News Word2Vec
* GloVe

###  Implemented
Data Collection and Cleaning
Modified Word Language Model
LSTM Classifier
Self-Attention Embedding
Key-Value Attention
Language Model Pretraining

### To Do
Classification Tuning
Attention Classification Tuning
Model Comparison

