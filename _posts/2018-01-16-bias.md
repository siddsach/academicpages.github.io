---
title: 'The Real AI Control Problem'
excerpt: "How Neural Attention can be used to Interpret bias in Word Embeddings<br/><img src='/images/164th_different_1324th_example.png'>"
date: 2018-01-16
permalink: /posts/2018/01/bias/
---

[Code](https://github.com/siddsach/Interpreting-Attention/)

Conversations about AI safety are all the rage these days. But the kinds of things people are afraid of are often uninformed by what is actually happening in A.I. We aren’t even close to general-purpose agents that can adapt to new environments or plan diverse sets of actions to realize abstract goals, and it’s not clear how long progress to that end will take. What’s much clearer is that for a wide range of problems, if you give a model massive amounts of training data
 and a clear objective to optimize, it can do surprisingly well. Face recognition or sentiment analysis used to be unthinkably difficult for computers, now the technology is becoming standard. But just because most people's conception of an AI apocalypse is warped, that doesn't mean machine learning is harmless.

A question I've been thinking a lot about recently is, when the training data is biased, what happens to these algorithms? Google recently caught some flak because its [sentiment analyzer classified sentences with the words “jew” and “gay” as negative](https://motherboard.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias). The case was significantly more concerning when a [ProPublica report](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) found that a machine-learning tool used by courts to predict a felon’s likelihood of committing another crime systematically discriminated against African-Americans. The ramifications of this problem aren’t theoretical or set to happen in the far future; they're happening right now. 

## Building a Subjectivity Classifier

I encountered this problem myself when I built a system for Flipside that would classify a sentence as an opinionated or factual claim. The goal of this algorithm wasn't to determine the veracity of a particular claim or be a "fake news detector"; it just aimed to determine whether the claim is asserting something as objectively true or expressing a subjective belief. Modern machine learning is effective enough that I was able to rope together some out-of-the-box methods and and reach human-level accuracy of 80% on a news subjectivity dataset<sup>1</sup>. Of course, my dataset necessarily contained some bias, whether because the annotators come from a particular demographic of people or because the sentences only cover particular topics. But these problems are at least partially fixable by getting a more diverse set of annotators and making more training data; not an easy task, but at least clear cut. 

What's more interesting is that my algorithm's effectiveness was grounded in using [word representations](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) trained on large datasets like Wikipedia and Google News. This is an example of a common practice in machine learning called [transfer learning](http://ruder.io/transfer-learning/), where models are pretrained on large datasets to learn low-level meaning to build on for high-level tasks. The problem is that the bias in this low-level knowledge  is much harder to correct. For example, if the best way to learn what words mean is by reading all of the internet, then you can't do much to correct the bias of the general population on the internet. I wanted to see whether the differences in my choice of word representations could actually lead to interpretable bias. Inspired by work on self-attention mechanisms (a new kind of neural memory module that uses a weighting over the tokens in an input to aggregate evidence), I wanted to try visualizing these weights to interpret what the model "sees", and by extension compare how different sources of ground-truth word meaning lead to different kinds of bias. It turns out, this works quite well. 

## Interpreting Attention

I used four settings: No word vectors (None), word vectors trained on Wikipedia (glove), word vectors trained on Google News (google), and word vectors I trained myself on Gigaword, an international news dataset (gigavec). An LSTM with Attention was trained on binary labels that were either subjective (True) or objective (false).

![alt text](/images/17th_different_4112th_example.png)

This is a good illustrative example. Here, it’s clear that each model is considering differing pieces of evidence to make its decision. Even though they both classify the sentence as subjective, the first model focuses on personal pronouns like “we” and “our” and the second model focuses on political words like “constitution” and “laws”, indicating different kinds of reasoning. The third model makes a different decision, paying attention to different parts of the sentence. It ignores the quotation mark and focuses on the generic pronoun "what", and thus
decides the sentence is subjective. In this way, we can see not only that the different models make different choices, we can form reasonable hypotheses as to the reasoning behind those decisions and how that reasoning differs between the differing "perspectives" the word vectors offer. Let's consider a few other examples.

![alt text](/images/39th_different_1619th_example.png)

In this case, the glove model demonstrates an ability to understand phrases in addition to words, as it’s able to recognize that the phrase “in line” is relevant to the sentence’s subjectivity, even though each word by itself is irrelevant to subjectivity. glove (Wikipedia) uses the biggest dataset, so this could be evidence of nuanced understanding its model has that the others don't. One also might notice that the model with word vectors (None) highlights most of the sentence. This
is likely because the model is memorizing parts of each example rather than actually understanding the sentence (that's why we use transfer learning!).

![alt text](/images/164th_different_1324th_example.png)

Just the phrase "global warming issues" caused the google news to classify the sentence as subjective, whereas the Wikipedia and International news dataset did the opposite. Just the volume of opinionated content in Google News that exists about global warming, causes the model to also learn that global warming is a subjective term.


![alt text](/images/51th_different_2409th_example.png)

Again, the google news model is showing some bias here. The words "iran" and "drug" were enough to trigger the model to classify it as subjective. These sorts of mistakes suggest that the model is using simple shortcuts to get likely correct rather than truly understanding the sentence. 


![alt text](/images/144th_different_2423th_example.png)

This example gives further credence to this idea. Simply because the sentence is about a scientific topic, both the Wikipedia and google news models fail to recognize the sentence as subjective. 

## So Biased Models are Biased. Now What?

These examples clearly demonstrate that data sources that are fairly representative of a wide range of domains and viewpoints can lead to socially harmful bias. However, some consistent patterns in these examples give hint to potential solutions.

* It seems many errors tend to occur because the model is doing simple pattern matching rather than operating from a global understanding of the sentence. Perhaps this can be remedied by using transfer learning not just to give a model meanings of words, but also to endow it with a more
global understanding of sentences of paragraphs. I'm working on using autoencoders and language models to do this.
* The kinds of reasoning the model uses is varied, but certainly falls into some set of buckets. In some cases, the topic defines the decisions such as the mention of Iran or Global Warming. In other cases, style and word choice is the key component such as usage of personal pronouns like "our" and "we". There are other such examples, but if our model can distinguish these sets of reasoning, , it'll be easier to understand what is going on. I'm working on clustering the hidden representations in some vector space and learning  mutually independent features using variational methods and then visualizing the attention weights of prototypical examples of each group/feature.

The problem of bias in machine learning is clearly a deep one (get it?). But I believe it can be fixed if we are less single-minded in focusing on accuracy rates and more considerate of the social nuances of applications of these methods. With a broader perspective, we can train our A.I. systems not only to excercise the breadth of human knowledge, but to internalize the nuance of our humanity as well.  



<sub><sup><sup>1</sup>Any conversation about applied machine learning has to start talk about the training data. I used an old academic dataset of sentences labeled by subjectivity because it contained news articles, the same domain my algorithm would have to perform in. Though these annotations highlighted subjective words and phrases rather than sentences, I was able to create pretty good sentence level annotations (80% human agreement) by checking whether a sentence had an intense subjective phrase
within it or not. I also cut out borderline cases with only mildly subjective phrases because if the human annotators are unsure about whether a sentence is subjective, the model definitely won’t understand (you can read more about this dataset here). But even with this data moxy, 10,000 labeled sentences just wasn’t enough training data for the model to perform at human-level. When I built an RNN Classifier on this data, it got a dismal 72% accuracy. Luckily, a common trick
of using pre-trained word representations helped me improve this to 81% accuracy.</sup></sub>
